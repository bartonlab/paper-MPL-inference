{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilation of test data for performance tests\n",
    "\n",
    "This notebook records the parameters for Wright-Fisher simulations used to generate our test data sets, as well as commands for running inference algorithms on the test data and compiling the results. In this work consider one **simple** and one **complex** scenario, intended to explore different regimes of computational and evolutionary complexity. We also consider an **example** that has somewhat more complicated evolutionary trajectories. \n",
    "\n",
    "Parameters for each of these scenarios are given below in **Section 1**.\n",
    "\n",
    "We analyzed the resulting trajectories with **7** different algorithms, in addition to MPL:\n",
    "\n",
    "1. FIT [[paper](https://doi.org/10.1534/genetics.113.158220)]  \n",
    "2. LLS [[paper](https://doi.org/10.1093/molbev/msx225)] [[code](https://github.com/ThomasTaus/poolSeq)]  \n",
    "3. CLEAR [[paper](https://doi.org/10.1534/genetics.116.197566)] [[code](https://github.com/airanmehr/CLEAR)]  \n",
    "4. EandR-timeseries [[paper](https://doi.org/10.1371/journal.pgen.1005069)] [[code](https://github.com/terhorst/EandR-timeseries)]  \n",
    "5. ApproxWF [[paper](https://doi.org/10.1534/genetics.115.184598)] [[code](https://bitbucket.org/phaentu/approxwf/wiki/Home)]  \n",
    "6. WFABC [[paper](https://doi.org/10.1111/1755-0998.12280)] [[code](http://jjensenlab.org/software)]  \n",
    "7. IM [[paper](https://doi.org/10.1534/genetics.111.133975)]  \n",
    "\n",
    "\n",
    "Here we have included a link to the code repository (if available) and to the corresponding paper. Scripts to run and compile output from these methods are collected in **Section 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries and define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was prepared using:\n",
      "python version 3.7.7 (default, Mar 10 2020, 15:43:33) \n",
      "[Clang 11.0.0 (clang-1100.0.33.17)]\n",
      "numpy version 1.18.4\n",
      "pandas version 1.0.3\n",
      "scikit-learn version 0.22.2.post1\n"
     ]
    }
   ],
   "source": [
    "# Full library list and version numbers\n",
    "\n",
    "print('This notebook was prepared using:')\n",
    "\n",
    "import sys\n",
    "print('python version %s' % sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print('numpy version %s' % np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas version %s' % pd.__version__)\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('scikit-learn version %s' % sk.__version__)\n",
    "\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "\n",
    "PBS_STR = \"\"\"#!/bin/bash\\n#PBS -m abe\\n#PBS -M jpbarton\\n#PBS -k oe\\n#PBS -j oe\\n#PBS -l nodes=1:ppn=1\\n\"\"\"\n",
    "\n",
    "# Code Ocean directories\n",
    "# WFS_DIR = '../data/wfsim'\n",
    "# WFS_DIR_REL = '../wfsim'\n",
    "# MPL_DIR = 'MPL'\n",
    "# SIM_MPL_DIR = '../data/simulation/MPL'\n",
    "# CLR_DIR = 'external/CLEAR'\n",
    "# EAR_DIR = 'external/EandR'\n",
    "# SIM_DIR = '../data/simulation'\n",
    "\n",
    "# GitHub directories\n",
    "WFS_DIR = 'src/wfsim'\n",
    "WFS_DIR_REL = '../wfsim'\n",
    "MPL_DIR = 'src/MPL'\n",
    "SIM_MPL_DIR = 'src/MPL/out'\n",
    "CLR_DIR = 'src/external/CLEAR'\n",
    "EAR_DIR = 'src/external/EandR'\n",
    "SIM_DIR = 'data/simulation'\n",
    "\n",
    "TESTS   = [   'example',      'medium_simple',      'medium_complex']\n",
    "N_VALS  = dict(example=  1000, medium_simple=  1000, medium_complex=1000)\n",
    "L_VALS  = dict(example=    50, medium_simple=    50, medium_complex=  50)\n",
    "T0_VALS = dict(example=     0, medium_simple=     0, medium_complex=  10)\n",
    "T_VALS  = dict(example=   400, medium_simple=  1000, medium_complex= 310)\n",
    "MU_VALS = dict(example=  1e-3, medium_simple=  1e-4, medium_complex=1e-4)\n",
    "NB_VALS = dict(example=    10, medium_simple=    10, medium_complex=  10)\n",
    "ND_VALS = dict(example=    10, medium_simple=    10, medium_complex=  10)\n",
    "SB_VALS = dict(example= 0.025, medium_simple= 0.025, medium_complex= 0.1)\n",
    "SD_VALS = dict(example=-0.025, medium_simple=-0.025, medium_complex=-0.1)\n",
    "\n",
    "N_TRIALS     =  100  # number of independent trials to run for each test set\n",
    "COMP_NS_VALS = [100] # number of sequence samples to collect per time point \n",
    "COMP_DT_VALS = [ 10] # time between sampling events (in discrete generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Generation of test data through Wright-Fisher simulations\n",
    "\n",
    "Wright-Fisher simulations are performed using `src/wfsim/Wright-Fisher.py`. The output of these simulations is saved for processing. The code below creates multiple job files for running many simulations in parallel on a computer cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example, 100x runs of\n",
    "# \n",
    "#     N   = 10^3         (population size)\n",
    "#     T   = 1000         (total number of generations to simulate)\n",
    "#     mu  = 1 x 10^{-3}  (mutation rate)\n",
    "#     L   = 50           (sequence length) \n",
    "#     n_b = 10           (number of beneficial mutations)\n",
    "#     n_d = 10           (number of deleterious mutations)\n",
    "#     s_b =  0.025       (selection coefficient for beneficial mutations)\n",
    "#     s_d = -0.025       (selection coefficient for deleterious mutations)\n",
    "#\n",
    "# RANDOM STARTING POPULATION (3 groups)\n",
    "\n",
    "test     = 'example'\n",
    "job_pars = {'-T'   : 1000,\n",
    "            '-N'   : N_VALS[test],\n",
    "            '-L'   : L_VALS[test],\n",
    "            '--mu' : MU_VALS[test],\n",
    "            '--nB' : NB_VALS[test],\n",
    "            '--fB' : SB_VALS[test],\n",
    "            '--nD' : ND_VALS[test],\n",
    "            '--fD' : SD_VALS[test],\n",
    "            '--random' : 3 }\n",
    "job_sub = open(WFS_DIR+'/jobs/run_wfsim_'+test+'.sh', 'w')\n",
    "for t in range(N_TRIALS):\n",
    "    trial_str = 'wfsim_'+test+'_%d' % t\n",
    "    job_sub.write('qsub -q verylong %s > /dev/null\\n' % ('jobs/'+trial_str+'.pbs'))\n",
    "    with open(WFS_DIR+'/jobs/'+trial_str+'.pbs', 'w') as f:\n",
    "        f.write(PBS_STR)\n",
    "        f.write('python3 Wright-Fisher.py -o data/%s ' % (trial_str))\n",
    "        f.write('%s\\n' % (' '.join([k + ' ' + str(v) for k, v in job_pars.items()])))\n",
    "job_sub.close()\n",
    "\n",
    "\n",
    "# MEDIUM SIMPLE, 100x runs of\n",
    "# \n",
    "#     N   = 10^3         (population size)\n",
    "#     T   = 10^4         (total number of generations to simulate)\n",
    "#     mu  = 1 x 10^{-4}  (mutation rate)\n",
    "#     L   = 50           (sequence length) \n",
    "#     n_b = 10           (number of beneficial mutations)\n",
    "#     n_d = 10           (number of deleterious mutations)\n",
    "#     s_b =  0.025       (selection coefficient for beneficial mutations)\n",
    "#     s_d = -0.025       (selection coefficient for deleterious mutations)\n",
    "\n",
    "test     = 'medium_simple'\n",
    "job_pars = {'-T'   : int(1.0e4),\n",
    "            '-N'   : N_VALS[test],\n",
    "            '-L'   : L_VALS[test],\n",
    "            '--mu' : MU_VALS[test],\n",
    "            '--nB' : NB_VALS[test],\n",
    "            '--fB' : SB_VALS[test],\n",
    "            '--nD' : ND_VALS[test],\n",
    "            '--fD' : SD_VALS[test] }\n",
    "job_sub = open(WFS_DIR+'/jobs/run_wfsim_'+test+'.sh', 'w')\n",
    "for t in range(N_TRIALS):\n",
    "    trial_str = 'wfsim_'+test+'_%d' % t\n",
    "    job_sub.write('qsub -q verylong %s > /dev/null\\n' % ('jobs/'+trial_str+'.pbs'))\n",
    "    with open(WFS_DIR+'/jobs/'+trial_str+'.pbs', 'w') as f:\n",
    "        f.write(PBS_STR)\n",
    "        f.write('python3 Wright-Fisher.py -o data/%s ' % (trial_str))\n",
    "        f.write('%s\\n' % (' '.join([k + ' ' + str(v) for k, v in job_pars.items()])))\n",
    "job_sub.close()\n",
    "\n",
    "\n",
    "# MEDIUM COMPLEX, 100x runs of\n",
    "# \n",
    "#     N   = 10^3         (population size)\n",
    "#     T   = 10^4         (total number of generations to simulate)\n",
    "#     mu  = 1 x 10^{-4}  (mutation rate)\n",
    "#     L   = 50           (sequence length) \n",
    "#     n_b = 10           (number of beneficial mutations)\n",
    "#     n_d = 10           (number of deleterious mutations)\n",
    "#     s_b =  0.100       (selection coefficient for beneficial mutations)\n",
    "#     s_d = -0.100       (selection coefficient for deleterious mutations)\n",
    "#\n",
    "# For these simulations the starting population is evenly split between\n",
    "# 5 collections of sequences with randomly chosen mutations (probability\n",
    "# of mutation is 50% at each site independent of other sites, \n",
    "# see Wright-Fisher.py for details)\n",
    "\n",
    "test     = 'medium_complex'\n",
    "job_pars = {'-T'   : int(1.0e4),\n",
    "            '-N'   : N_VALS[test],\n",
    "            '-L'   : L_VALS[test],\n",
    "            '--mu' : MU_VALS[test],\n",
    "            '--nB' : NB_VALS[test],\n",
    "            '--fB' : SB_VALS[test],\n",
    "            '--nD' : ND_VALS[test],\n",
    "            '--fD' : SD_VALS[test],\n",
    "            '--random' : 5 }\n",
    "job_sub = open(WFS_DIR+'/jobs/run_wfsim_'+test+'.sh', 'w')\n",
    "for t in range(N_TRIALS):\n",
    "    trial_str = 'wfsim_'+test+'_%d' % t\n",
    "    job_sub.write('qsub -q verylong %s > /dev/null\\n' % ('jobs/'+trial_str+'.pbs'))\n",
    "    with open(WFS_DIR+'/jobs/'+trial_str+'.pbs', 'w') as f:\n",
    "        f.write(PBS_STR)\n",
    "        f.write('python3 Wright-Fisher.py -o data/%s ' % (trial_str))\n",
    "        f.write('%s\\n' % (' '.join([k + ' ' + str(v) for k, v in job_pars.items()])))\n",
    "job_sub.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Wright-Fisher trajectories have been generated, we subsample them to create our test trajectories using `src/wfsim/py2c.py`. For comparison between inference methods we chose to take 100 sequences per sample, with samples taken every 10 generations. The starting and ending generations of these test trajectories are\n",
    "\n",
    "1. example -- start 0, end 400\n",
    "2. medium simple -- start 0, end 1000\n",
    "3. medium complex -- start 10, end 310\n",
    "\n",
    "The code below produces 3 shell scripts `expand_example.sh`, `expand_medium_simple.sh`, and `expand_medium_complex.sh`, which can be run to extract the trajectories from the compressed output of `src/wfsim/Wright-Fisher.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Running the inference algorithms and compiling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sub-trajectories from full samples\n",
    "\n",
    "ns_vals = [10, 20, 30, 40, 50,  80, 100, 1000]\n",
    "dt_vals = [ 1,  5, 10, 20, 50]\n",
    "\n",
    "for t in TESTS[1:]:\n",
    "    job_sub = open('%s/expand_%s.sh' % (WFS_DIR, t), 'w')\n",
    "    for ns in COMP_NS_VALS:\n",
    "        for dt in COMP_DT_VALS:\n",
    "            for i in range(N_TRIALS):\n",
    "                job_sub.write('python3 py2c.py -i data/wfsim_%s_%d -t %d -T %d --ns %d --dt %d -s %d\\n' \n",
    "                            % (t, i, T0_VALS[t], T_VALS[t], ns, dt, i))\n",
    "    job_sub.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPL\n",
    "\n",
    "First create the job files and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_vals = [10, 20, 30, 40, 50,  80, 100, 1000]\n",
    "dt_vals = [ 1,  5, 10, 20, 50]\n",
    "\n",
    "for t in TESTS:\n",
    "    job_sub = open('%s/jobs/run_wfinf_%s.sh' % (MPL_DIR, t), 'w')\n",
    "    job_sub.write('g++ src/main.cpp src/inf-binary.cpp src/io.cpp -O3 ')\n",
    "    job_sub.write('-march=native -lgslcblas -lgsl -o bin/mpl-binary\\n')\n",
    "    for ns in ns_vals:\n",
    "        for dt in dt_vals:\n",
    "            trial_str = 'wfinf_%s_T%d_ns%d_dt%d' % (t, T_VALS[t], ns, dt)\n",
    "            job_sub.write('qsub -q verylong %s > /dev/null\\n' % ('jobs/'+trial_str+'.pbs'))\n",
    "            with open(MPL_DIR+'/jobs/'+trial_str+'.pbs', 'w') as f:\n",
    "                f.write(PBS_STR)\n",
    "                for i in range(N_TRIALS):\n",
    "                    i_str = '%s/data/wfsim_%s_%d_T%d_ns%d_dt%d' % (WFS_DIR_REL, t, i, T_VALS[t], ns, dt)\n",
    "                    o_str = 'out/%s_%d_T%d_ns%d_dt%d'           % (             t, i, T_VALS[t], ns, dt)\n",
    "                    f.write('python3 %s/py2c.py -i %s/data/wfsim_%s_%d -t %d -T %d --ns %d --dt %d -s %d\\n' \n",
    "                            % (WFS_DIR_REL, WFS_DIR_REL, t, i, T0_VALS[t], T_VALS[t], ns, dt, i))\n",
    "                    if ns in COMP_NS_VALS and dt in COMP_DT_VALS:\n",
    "                        f.write('./bin/mpl-binary -i %s.dat -o %s_MPL.dat'    % (i_str, o_str))\n",
    "                        f.write(' -g 1e3 -N %d -mu %.3e > %s_MPL_time.dat\\n'  % (N_VALS[t], MU_VALS[t], o_str))\n",
    "                        f.write('./bin/mpl-binary -i %s.dat -o %s_SL.dat -nc' % (i_str, o_str))\n",
    "                        f.write(' -g 1e3 -N %d -mu %.3e > %s_SL_time.dat\\n'   % (N_VALS[t], MU_VALS[t], o_str))\n",
    "                    else:\n",
    "                        f.write('./bin/mpl-binary -i %s.dat -o %s_MPL.dat' % (i_str, o_str))\n",
    "                        f.write(' -g 1e3 -N %d -mu %.3e > /dev/null\\n'     % (N_VALS[t], MU_VALS[t]))\n",
    "                        f.write('./bin/mpl-binary -i %s.dat -o %s_SL.dat'  % (i_str, o_str))\n",
    "                        f.write(' -nc -g 1e3 -N %d -mu %.3e > /dev/null\\n' % (N_VALS[t], MU_VALS[t]))\n",
    "                    f.write('./bin/mpl-binary -i %s.dat -o %s_MPL_noMu.dat' % (i_str, o_str))\n",
    "                    f.write(' -g 1e3 -N %d -mu 0 > /dev/null\\n'             % (N_VALS[t]))\n",
    "                    f.write('./bin/mpl-binary -i %s.dat -o %s_SL_noMu.dat'  % (i_str, o_str))\n",
    "                    f.write(' -nc -g 1e3 -N %d -mu 0 > /dev/null\\n'         % (N_VALS[t]))\n",
    "                    if t=='example' and ns==1000 and dt==1 and i==0:\n",
    "                        f.write('./bin/mpl-binary -i %s.dat -o %s_MPL.dat'    % (i_str, o_str))\n",
    "                        f.write(' -g 1e3 -N %d -mu %.3e -sc %s > /dev/null\\n' \n",
    "                                % (N_VALS[t], MU_VALS[t], \n",
    "                                   o_str.split('/')[0]+'/covariance-'+o_str.split('/')[1]+'.dat'))\n",
    "                    else:\n",
    "                        f.write('rm %s.dat\\n' % i_str)\n",
    "    job_sub.close()\n",
    "\n",
    "    methods = ['MPL', 'SL', 'MPL_noMu', 'SL_noMu']\n",
    "\n",
    "    job_collect = open('%s/jobs/run_wfinf_%s_collect.sh' % (MPL_DIR, t), 'w')\n",
    "    job_collect.write('python3 collect_s.py -i out/%s -n %d -T %d -t %d' % (t, N_TRIALS, T_VALS[t], T0_VALS[t]))\n",
    "    for  m in methods: job_collect.write(  ' -m %s' %  m)\n",
    "    for ns in ns_vals: job_collect.write(' --ns %d' % ns)\n",
    "    for dt in dt_vals: job_collect.write(' --dt %d' % dt)\n",
    "    job_collect.write(' &\\n')\n",
    "    job_collect.write('cd %s/out && tar czf %s.tar.gz ' % (MPL_DIR, t))\n",
    "    for m in methods: job_collect.write(' *%s*_%s.dat' % (t, m))\n",
    "    job_collect.write(' && cd ../..')\n",
    "    job_collect.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next collect and organize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in TESTS:\n",
    "    true_ben = [1 if i in                       range(NB_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_del = [1 if i in  range(L_VALS[t]-ND_VALS[t], L_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_neu = [1 if i in range(NB_VALS[t], L_VALS[t]-ND_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    coefs    = ['s%d' % j for j in range(L_VALS[t])]\n",
    "    \n",
    "    df              = pd.read_csv('%s/%s_collected.csv' % (SIM_MPL_DIR, t), memory_map=True)\n",
    "    df['AUROC_ben'] = pd.Series(data=[roc_auc_score(true_ben, np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    df['AUROC_del'] = pd.Series(data=[roc_auc_score(true_del,-np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    for i in range(L_VALS[t]):\n",
    "        if   true_ben[i]: df['ds%d' % i] = df['s%d' % i] - SB_VALS[t]\n",
    "        elif true_del[i]: df['ds%d' % i] = df['s%d' % i] - SD_VALS[t]\n",
    "        elif true_neu[i]: df['ds%d' % i] = df['s%d' % i]\n",
    "    \n",
    "    df.to_csv('%s/MPL_%s_collected_extended.csv.gz' % (SIM_DIR, t), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1, 5-7. FIT, ApproxWF, WFABC, and IM\n",
    "\n",
    "See Matlab scripts in the `src/Matlab/` directory for data processing and running the FIT, ApproxWF, WFABC, and IM inference routines. An overview of this analysis is presented in `src/Matlab/README.TXT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LLS\n",
    "\n",
    "See the R script in the `src/R` directory for data processing and running the LLS inference routine. An overview of this analysis is presented in `src/R/README.TXT`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CLEAR\n",
    "\n",
    "First create the job files and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbs_str_clr = PBS_STR + 'START=$(date +\"%s.%N\")\\n'\n",
    "pbs_end     = 'RUNTIME=$(echo \"$(date +%s.%N) - $START\" | bc)\\necho \"$RUNTIME\" >> '\n",
    "\n",
    "for t in TESTS:\n",
    "    if t=='example':\n",
    "        continue\n",
    "    job_sub = open('%s/jobs/run_%s.sh' % (CLR_DIR, t), 'w')\n",
    "    for ns in COMP_NS_VALS:\n",
    "        for dt in COMP_DT_VALS:\n",
    "            for i in range(N_TRIALS):\n",
    "                _data     = np.loadtxt('%s/data/wfsim_%s_%d_T%d_ns%d_dt%d.dat' % (WFS_DIR, t, i, T_VALS[t], ns, dt))\n",
    "                _L        = len(_data[0][2:])\n",
    "                times     = np.unique(_data.T[0])\n",
    "                positions = np.array(range(1, _L+1),int)\n",
    "\n",
    "                levels     = [[1], [int(_t) for _t in times], ['C', 'D']]\n",
    "                names      = ['REP', 'GEN', 'READ']\n",
    "                indices    = ['CHROM', 'POS']\n",
    "                col_values = {}\n",
    "                col_tuples = []\n",
    "                idx_tuples = [('chrI', l+1) for l in range(_L)]\n",
    "\n",
    "                for j in range(len(times)):\n",
    "                    _t_data = np.array([_d[2:] for _d in _data if _d[0]==times[j]])\n",
    "                    _t_num  = np.array([ _d[1] for _d in _data if _d[0]==times[j]])\n",
    "                    _t_sum  = np.einsum('i,ij->j', _t_num, _t_data)\n",
    "                    for l in range(_L):\n",
    "                        col_tuples.append((1, int(times[j]), 'C'))\n",
    "                        col_tuples.append((1, int(times[j]), 'D'))\n",
    "                        if (1, int(times[j]), 'C') in col_values:\n",
    "                            col_values[(1, int(times[j]), 'C')].append(_t_sum[l]+1)\n",
    "                            col_values[(1, int(times[j]), 'D')].append(np.sum(_t_num)+1)\n",
    "                        else:\n",
    "                            col_values[(1, int(times[j]), 'C')] = [_t_sum[l]+1]\n",
    "                            col_values[(1, int(times[j]), 'D')] = [np.sum(_t_num)+1]\n",
    "\n",
    "                df_CLEAR = pd.DataFrame(col_values, index = np.array(range(_L),int)+1)\n",
    "                df_CLEAR.columns.names = tuple(names)\n",
    "                df_CLEAR.to_pickle('%s/data/%s_%d.df' % (CLR_DIR, t, i))\n",
    "                \n",
    "                o_str = '%s_%d' % (t, i)\n",
    "                with open('%s/jobs/%s.pbs' % (CLR_DIR, o_str), 'w') as f:\n",
    "                    f.write(pbs_str_clr)\n",
    "                    f.write('python3 %s/CLEAR.py --pandas %s/data/%s.df' % (CLR_DIR, CLR_DIR, o_str))\n",
    "                    f.write(' --N %d --out %s/out/%s.df\\n'               % (N_VALS[t], CLR_DIR, o_str))\n",
    "                    f.write('%s%s/out/%s_time.dat\\n'                     % (pbs_end, CLR_DIR, o_str))\n",
    "                \n",
    "                job_sub.write('qsub -q verylong %s/jobs/%s_%d.pbs > /dev/null\\n' % (CLR_DIR, t, i))\n",
    "                \n",
    "    job_sub.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next collect and organize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process CLEAR results\n",
    "\n",
    "for t in TESTS:\n",
    "    if t=='example':\n",
    "        continue\n",
    "    true_ben = [1 if i in                       range(NB_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_del = [1 if i in  range(L_VALS[t]-ND_VALS[t], L_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_neu = [1 if i in range(NB_VALS[t], L_VALS[t]-ND_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    coefs    = ['s%d' % j for j in range(L_VALS[t])]\n",
    "    \n",
    "    f    = open('%s/out/%s_collected.csv' % (CLR_DIR, t), 'w')\n",
    "    head = 'trajectory,method,t0,T,ns,deltat,runtime,' + (','.join(coefs))\n",
    "    f.write('%s\\n' % head)\n",
    "    \n",
    "    for n in range(N_TRIALS):\n",
    "        temp_df = pd.melt(pd.read_pickle('%s/out/%s_%d.df' % (CLR_DIR, t, n)))\n",
    "        temp_s  = np.array(temp_df[temp_df.stat=='s'].value)\n",
    "        temp_t  = float([i.split() for i in open('%s/out/%s_%d_time.dat' % (CLR_DIR, t, n)).readlines()][-1][0])\n",
    "        \n",
    "        f.write('%d,%s,%d,%d,%d,%d,%lf,' % (n, 'CLEAR', T0_VALS[t], T_VALS[t], ns, dt, temp_t))\n",
    "        f.write(','.join(['%lf' % s for s in temp_s]))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    df              = pd.read_csv('%s/out/%s_collected.csv' % (CLR_DIR, t), memory_map=True)\n",
    "    df['AUROC_ben'] = pd.Series(data=[roc_auc_score(true_ben, np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    df['AUROC_del'] = pd.Series(data=[roc_auc_score(true_del,-np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    for i in range(L_VALS[t]):\n",
    "        if   true_ben[i]: df['ds%d' % i] = df['s%d' % i] - SB_VALS[t]\n",
    "        elif true_del[i]: df['ds%d' % i] = df['s%d' % i] - SD_VALS[t]\n",
    "        elif true_neu[i]: df['ds%d' % i] = df['s%d' % i]\n",
    "        \n",
    "    df.to_csv('%s/CLEAR_%s_collected_extended.csv.gz' % (SIM_DIR, t), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. EandR-timeseries\n",
    "\n",
    "First create the job files and run them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbs_str_ear = \"\"\"#!/bin/bash\\n#PBS -m abe\\n#PBS -M jpbarton\\n#PBS -k oe\\n#PBS -j oe\\n#PBS -l nodes=1:ppn=4\\n\"\"\"\n",
    "pbs_str_ear = pbs_str_ear + 'START=$(date +\"%s.%N\")\\n'\n",
    "pbs_end     = 'RUNTIME=$(echo \"$(date +%s.%N) - $START\" | bc)\\necho \"$RUNTIME\" >> '\n",
    "\n",
    "for t in TESTS:\n",
    "    if t=='example':\n",
    "        continue\n",
    "    job_sub = open('%s/jobs/run_%s.sh' % (EAR_DIR, t), 'w')\n",
    "    for ns in COMP_NS_VALS:\n",
    "        for dt in COMP_DT_VALS:\n",
    "            for i in range(N_TRIALS):\n",
    "                o_str = '%s/out/%s_%d' % (EAR_DIR, t, i)\n",
    "                i_str = '%s/data/wfsim_%s_%d_T%d_ns%d_dt%d.dat' % (WFS_DIR, t, i, T_VALS[t], ns, dt)\n",
    "                with open('%s/jobs/%s_%d.pbs' % (EAR_DIR, t, i), 'w') as f:\n",
    "                    f.write(pbs_str_ear)\n",
    "                    f.write('python3 %s/EandR.py -N %d -i %s -o %s.dat\\n' % (EAR_DIR, N_VALS[t], i_str, o_str))\n",
    "                    f.write('%s%s_time.dat\\n'                             % (pbs_end, o_str))\n",
    "                    job_sub.write('qsub -q verylong %s/jobs/%s_%d.pbs > /dev/null\\n' % (EAR_DIR, t, i))\n",
    "    job_sub.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next collect and organize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in TESTS:\n",
    "    if t=='example':\n",
    "        continue\n",
    "    true_ben = [1 if i in                       range(NB_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_del = [1 if i in  range(L_VALS[t]-ND_VALS[t], L_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    true_neu = [1 if i in range(NB_VALS[t], L_VALS[t]-ND_VALS[t]) else 0 for i in range(L_VALS[t])]\n",
    "    coefs    = ['s%d' % j for j in range(L_VALS[t])]\n",
    "    \n",
    "    f    = open('%s/out/%s_collected.csv' % (EAR_DIR, t), 'w')\n",
    "    head = 'trajectory,method,t0,T,ns,deltat,runtime,' + (','.join(coefs))\n",
    "    f.write('%s\\n' % head)\n",
    "    \n",
    "    for ns in COMP_NS_VALS:\n",
    "        for dt in COMP_DT_VALS:\n",
    "            for n in range(N_TRIALS):\n",
    "                temp_s = np.loadtxt('%s/out/%s_%d.dat' % (EAR_DIR, t, n))\n",
    "                temp_t = np.loadtxt('%s/out/%s_%d_time.dat' % (EAR_DIR, t, n))\n",
    "                if temp_t.shape!=(): temp_t = temp_t[-1]\n",
    "                \n",
    "                f.write('%d,%s,%d,%d,%d,%d,%lf,' % (n, 'EandR', T0_VALS[t], T_VALS[t], ns, dt, temp_t))\n",
    "                f.write(','.join(['%lf' % s for s in temp_s]))\n",
    "                f.write('\\n')\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    df              = pd.read_csv('%s/out/%s_collected.csv' % (EAR_DIR, t), memory_map=True)\n",
    "    df['AUROC_ben'] = pd.Series(data=[roc_auc_score(true_ben, np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    df['AUROC_del'] = pd.Series(data=[roc_auc_score(true_del,-np.array(df.iloc[i][coefs])) for i in range(len(df))])\n",
    "    for i in range(L_VALS[t]):\n",
    "        if   true_ben[i]: df['ds%d' % i] = df['s%d' % i] - SB_VALS[t]\n",
    "        elif true_del[i]: df['ds%d' % i] = df['s%d' % i] - SD_VALS[t]\n",
    "        elif true_neu[i]: df['ds%d' % i] = df['s%d' % i]\n",
    "            \n",
    "    df.to_csv('%s/EandR_%s_collected_extended.csv.gz' % (SIM_DIR, t), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
